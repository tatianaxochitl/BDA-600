<!DOCTYPE html>
<html lang="en">
  <head>
    <title>ARIMA</title>
    <script src="https://code.jquery.com/jquery-3.5.0.js"></script>
    <script>
      $(function () {
        $("#header").load("includes/header.html");
        $("#footer").load("includes/footer.html");
      });
    </script>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="includes/style.css" />
    <body>
      <div id="header"></div>
      <main>
        <h1>ARIMA Model</h1>
        <p>
          <strong>A</strong>uto <strong>R</strong>egressive
          <strong>I</strong>ntegrated <strong>M</strong>oving <strong>A</strong>verage, also known as
          <strong>ARIMA</strong> is a model primarily used for time series forecasting. The model focuses
          on Autocorrelations in the data, which is the degree of similarity
          between a time series and a lagged version of itself over succesive time intervals.

        </p>
        <h2>Three Variables to Account for:</h2>
        <p>
          <strong>P</strong> = The periods of lag used. For example, P = 5 would use five previous periods of the time series
          in the Auto Regressive portion of the model.
        <br>
        <br>
          Note: A purely Auto Regressive model will look more like a linear regression.
        <br>
        <br>
          <strong>D</strong> = The number of differencing transformations used to make the 
          tim series stationary.
        <br>
        <br>
          Note: When it comes to predictive modeling of time series data, Stationarity is the idea that the 
          mean and the variance are constant over time. This makes for easier prediction. The process of "differencing" starts by taking the difference of the 
          current and previous time periods. If the resulting values do not center around a constant mean or variance, then
          differencing is continued using the previous period and the first difference.
        <br>
        <br>
          <strong>Q</strong> = The lag of error component where the component is a part of the time series not
          explained by trend or seasonality.
        <br>
        <br>
          Note: 
        </p>
        <h2>ADF</h2>
        <p>
        The ADF (Augmented Dickey Fuller Test) was the first step in the manual process of determining the 
        parameters needed for the ARIMA model. By using the resulting p-value from the test:
        <br><br>
        P < 0.05 then stationary 
        <br>
        P > 0.05 then not stationary
        <br><br>
        If the time series is non stationary we can adjust the D parameter by 1 to difference the data.
        <br><br>
        When conducting this test on Apple's Adjusted Closing Price time series, the resulting output:
        <br><br>
        1. ADF :  0.18009757213664973<br>
        2. P-Value :  0.9711229737500696<br>
        3. Num Of Lags :  20<br>
        4. Num Of Observations Used For ADF Regression: 1026<br>
        5. Critical Values :<br>
        &emsp; &emsp;  1% :  -3.4367396063176874<br>
        &emsp; &emsp;  5% :  -2.8643611157329905<br>
        &emsp; &emsp;  10% :  -2.5682720836420705<br>
   <br><br>
   tells us that the series in non stationary and we should adjust the D paramter. We will use a one time difference (1).
        </p>
        <h2>ACF & PACF</h2>
        <p>
        The ACF (Auto Correlation Function Plot) is used to see the correlation between the points,
        up to and including the lag unit. The correlation coefficient is shown on the x-axis while the 
        y-axis is the number of lags.<br><br>
        If there is a Positive autocorrelation at lag 1 then we use the AR model<br>
        If there is a Negative autocorrelation at lag 1 then we use the MA model<br><br>
        As you can see with the lags of the Apple Close Prices, there is a positive autocorrelation
        at lag 1 so we will use the AR model. <br><br>Now we can look at the PACF (Partial AutoCorrelation Function Plot) which
        summarizes the relationships between an observation in the time series and observations at prior steps 
        with the relationships of intervening observations removed. <br><br>
        If the plot drops off at a certain lag then you will use the AR(n) model; if the drop is more gradual
        then you will use the MA model.<br><br>
        As you can see from the PACF of Apple, we should be using the AR(1) model as a parameter.
        </p>
        <h2>Forecasting</h2>
        <p>
        Now with the stationarity and autocorrelation information taken care of we have our three parmeters for the ARIMA model.
        We have 1 for our P parameter (ACF & PACF) and 1 for our D parameter (Stationarity). We leave the Q parameter at 0 since we 
        have decided to use the AutoRegressive model over the Moving Average model based off of ACF and PACF tests.<br><br>
        After splitting the time series into training and test sets, we can use the ARIMA function in Python combined with the order (parameters) we
        just discovered (1, 1, 0) to fit the model. Below you see a 30 day forecast for Apple laid over the actual prices on those same days. What we observed is that
        the prediction model does not fully coincide with the actual values but seemed to be very accurate with up and down trends in the time series.
        </p>
        <h2>Plots of Forecasts</h2>
        <p>
        Plots
        </p>  
    </body>
  </head>
</html>
